{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/d/DKE/MRP1_SizeMatters/Size-Matters/framework\")\n",
    "sys.path.append(\"/mnt/d/DKE/MRP1_SizeMatters/Size-Matters/application\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from captum.attr import LayerConductance\n",
    "\n",
    "import train_utils\n",
    "from NetworkClass import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model from Network Class and Define Dataloaders for MNIST from torchvision\n",
    "Batch size test can be varied to generate conductance values over a larger set of inputs. Train batch can also be changed for faster learning within an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "random_seed = 1\n",
    "\n",
    "    \n",
    "model_dict = {\n",
    "        \"network\":{\n",
    "            'input_layer': {\n",
    "                \"units\": 784,\n",
    "                \n",
    "                },\n",
    "            'hidden_layer': [{\n",
    "                    \"units\": 500, \n",
    "                    \"activation\": \"relu\",\n",
    "                    \"type\": \"Linear\"\n",
    "                }, \n",
    "                {\n",
    "                    \"units\": 300, \n",
    "                    \"activation\": \"relu\",\n",
    "                    \"type\": \"Linear\"\n",
    "\n",
    "                }],\n",
    "            'output_layer': {\n",
    "                \"units\": 10,\n",
    "                \"activation\": \"softmax\",\n",
    "                \"type\": \"Linear\"\n",
    "                }\n",
    "        }\n",
    "    }\n",
    "\n",
    "model = Network(model_dict)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,)), train_utils.ReshapeTransform((-1,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,)), train_utils.ReshapeTransform((-1,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0186,  0.0290, -0.0332,  ...,  0.0328, -0.0356,  0.0355],\n",
      "        [-0.0061, -0.0219,  0.0274,  ...,  0.0039,  0.0194,  0.0053],\n",
      "        [ 0.0270,  0.0123,  0.0148,  ..., -0.0249, -0.0182, -0.0018],\n",
      "        ...,\n",
      "        [-0.0028, -0.0174, -0.0341,  ...,  0.0197,  0.0055,  0.0215],\n",
      "        [ 0.0111, -0.0177,  0.0152,  ..., -0.0075,  0.0078,  0.0285],\n",
      "        [ 0.0185, -0.0341,  0.0150,  ...,  0.0096, -0.0185,  0.0289]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 3.0783e-02, -1.8658e-02,  3.4878e-03, -1.2988e-02, -3.3305e-02,\n",
      "        -1.4899e-02, -2.3604e-02, -6.0555e-03,  2.1412e-02,  1.4235e-03,\n",
      "        -1.6945e-02, -3.4793e-02,  3.2999e-02,  4.2396e-03, -9.0187e-03,\n",
      "        -6.9939e-03, -3.0228e-02,  7.8130e-03,  4.8421e-04, -3.0152e-03,\n",
      "        -3.1360e-02,  1.7814e-04,  4.9348e-03,  3.2803e-02,  1.1678e-02,\n",
      "        -2.9558e-02,  3.2571e-02,  2.9897e-02,  2.7514e-02,  9.5702e-03,\n",
      "        -2.6003e-02, -3.5166e-02, -8.9526e-03,  2.3678e-02, -1.0270e-02,\n",
      "         2.4056e-02, -2.4355e-02,  1.9106e-02, -3.3063e-02, -2.4864e-03,\n",
      "         2.9479e-02,  1.6737e-02,  1.5891e-02, -2.4454e-02, -1.6448e-03,\n",
      "         1.5202e-02,  3.4507e-02,  7.3826e-03,  3.3322e-03,  3.2648e-02,\n",
      "         3.4844e-02,  2.5739e-02, -2.9303e-02,  3.0909e-02,  4.6049e-03,\n",
      "        -1.3450e-02, -1.9802e-02, -3.3844e-02,  1.7998e-02,  3.1882e-02,\n",
      "         4.6986e-03,  1.4506e-04, -1.0490e-03,  3.5478e-02, -1.0558e-02,\n",
      "        -3.1823e-02,  4.6986e-03,  1.0376e-02,  1.4627e-02, -1.8663e-02,\n",
      "        -3.1956e-02,  9.6123e-03, -3.3617e-03,  3.0684e-02, -1.8862e-02,\n",
      "         3.4164e-02,  3.0640e-02,  1.8582e-02,  2.0184e-02, -2.4924e-02,\n",
      "         2.8008e-02, -3.4053e-02,  6.9253e-03, -1.6711e-02,  3.1772e-02,\n",
      "         2.3348e-02,  1.2640e-02, -8.2599e-03,  3.0345e-02, -4.5792e-03,\n",
      "         5.3986e-03,  6.7912e-03,  2.0615e-02, -1.1755e-02, -1.4226e-02,\n",
      "        -2.3514e-02, -9.2664e-03,  2.8551e-02,  1.5514e-03,  1.3828e-02,\n",
      "        -1.2485e-02, -2.8115e-05, -2.6163e-02,  2.5237e-02, -1.9007e-02,\n",
      "         9.9429e-03,  2.9086e-02,  2.5197e-03,  6.7738e-03,  2.1000e-02,\n",
      "        -2.6522e-02,  1.4080e-02,  1.4759e-02, -2.6037e-02, -2.7015e-02,\n",
      "        -1.0057e-03,  6.5367e-03, -3.5395e-02,  2.4260e-02, -2.2282e-02,\n",
      "         1.2578e-02,  1.8235e-03, -3.3775e-02,  1.9189e-02,  3.2317e-02,\n",
      "        -2.5847e-02, -2.2062e-03,  2.9962e-03, -3.3287e-02, -4.9141e-03,\n",
      "        -1.0978e-02, -3.1938e-02, -2.3240e-02,  1.4193e-02, -2.1409e-02,\n",
      "        -3.2494e-02,  2.7576e-02,  2.6156e-03,  1.2784e-04,  1.2209e-02,\n",
      "        -3.1248e-02,  2.6494e-02, -1.6548e-02, -2.8686e-02, -1.8669e-02,\n",
      "         2.8491e-02, -1.4826e-02, -7.3171e-03, -2.8153e-02, -1.4060e-02,\n",
      "         7.1709e-03, -3.1103e-02,  3.0478e-02, -8.0599e-04,  1.9595e-03,\n",
      "        -2.4512e-02, -2.9569e-02,  2.1816e-02, -6.8489e-03, -2.4817e-02,\n",
      "         1.8110e-02,  3.2195e-03,  2.8837e-02,  1.1492e-02, -1.6229e-02,\n",
      "         3.3852e-02, -1.0556e-02,  3.5338e-02,  2.7258e-03,  7.2395e-03,\n",
      "         2.6078e-02,  1.9853e-02,  5.1680e-03,  9.8226e-03,  2.1087e-02,\n",
      "         1.4585e-02, -4.7451e-03, -1.3617e-02, -9.1264e-03,  2.7485e-02,\n",
      "         4.0054e-03, -2.4286e-02, -5.8933e-03, -3.2004e-02,  1.1290e-02,\n",
      "         2.7963e-02, -7.7741e-03, -2.5268e-03, -4.7605e-03,  1.4154e-03,\n",
      "         2.9442e-02,  1.2000e-02, -1.6500e-02,  3.3108e-04, -2.8325e-03,\n",
      "         2.5915e-02, -1.9341e-02,  2.9264e-02,  8.2037e-03,  3.2811e-02,\n",
      "         1.9245e-02,  2.6591e-02,  8.3832e-04,  8.9666e-03, -2.1564e-02,\n",
      "        -6.3852e-03, -2.7859e-02,  3.1034e-02,  3.6916e-03, -2.5202e-02,\n",
      "        -2.9262e-02, -2.6818e-02,  4.8163e-03,  6.5282e-03, -5.1223e-03,\n",
      "         6.1079e-03, -3.0046e-02, -1.5877e-02,  1.6479e-02, -3.2887e-02,\n",
      "         3.8415e-03,  3.2498e-02,  3.1484e-02,  8.8532e-03,  3.4996e-03,\n",
      "         2.5410e-02,  3.0876e-02, -2.2472e-02, -1.8699e-02, -1.5216e-02,\n",
      "         1.8377e-02, -9.0717e-03, -2.5551e-02,  8.7317e-03, -2.3125e-02,\n",
      "         2.7437e-02,  1.3917e-02, -2.9324e-02, -2.0777e-03,  2.3104e-03,\n",
      "        -6.9496e-03, -6.5046e-03,  2.6827e-02, -2.1640e-02,  3.2203e-02,\n",
      "        -3.1335e-02,  6.3827e-03,  3.5424e-02, -3.3435e-02,  1.6616e-02,\n",
      "         2.1777e-03,  3.5672e-02, -1.8255e-02,  8.8830e-03,  1.3239e-02,\n",
      "         1.1710e-02,  2.6484e-02, -2.4657e-03,  6.2664e-03, -2.3949e-03,\n",
      "        -7.4659e-04,  2.8041e-02, -1.8372e-02,  5.4377e-03, -8.9726e-03,\n",
      "         1.3553e-02,  3.2962e-02,  1.8089e-02, -2.6301e-02, -7.6218e-03,\n",
      "        -5.8168e-03, -1.2706e-03, -1.8372e-02, -6.3734e-03, -1.1739e-02,\n",
      "         6.2019e-04,  1.0338e-02, -2.2707e-02, -6.4910e-03,  1.4813e-02,\n",
      "        -1.2754e-02,  1.8495e-02, -1.5436e-02,  1.0786e-02, -2.4324e-02,\n",
      "         2.2293e-02, -3.1405e-02,  2.1074e-02, -1.4285e-02,  1.0453e-02,\n",
      "        -1.7663e-02,  2.7028e-02, -1.2920e-02, -4.4154e-04,  1.0121e-02,\n",
      "         1.3049e-02,  2.2208e-02,  5.3051e-04,  1.2183e-02, -1.9951e-03,\n",
      "        -1.9344e-02, -2.7358e-02, -2.6361e-02,  1.9034e-03,  2.9898e-02,\n",
      "         2.7231e-02,  3.4880e-03, -6.2650e-04, -6.9433e-03, -2.2121e-02,\n",
      "        -3.3167e-02,  3.5246e-02, -2.4889e-02,  1.3974e-02,  2.1963e-02,\n",
      "         2.7870e-02, -2.3355e-02,  7.9111e-03,  1.5721e-02,  3.4082e-02,\n",
      "        -1.0191e-02,  2.3543e-03,  3.1754e-02, -3.1609e-02,  2.0298e-02,\n",
      "        -1.9425e-02, -1.8684e-02, -2.5547e-02, -2.3890e-02, -4.4056e-03,\n",
      "        -1.6878e-03,  2.4832e-02,  4.3012e-03,  1.1667e-02,  1.0811e-02,\n",
      "         1.5090e-02,  1.4127e-02,  6.7588e-03, -7.6327e-03, -2.2145e-03,\n",
      "        -3.3248e-02,  1.8512e-03,  9.9460e-03, -2.3390e-02,  2.6688e-02,\n",
      "        -3.0105e-02,  1.0008e-02, -3.1249e-02, -2.6369e-02, -1.0420e-02,\n",
      "         7.7969e-03, -2.1388e-03, -5.6139e-03,  3.4299e-02, -1.2291e-02,\n",
      "         4.1819e-03, -3.6078e-03,  1.2825e-02,  3.4667e-02, -1.1972e-02,\n",
      "        -1.5568e-02, -2.7611e-02,  7.0107e-03, -1.7813e-02,  1.2862e-02,\n",
      "         2.4211e-02, -1.7453e-02,  1.6890e-02, -2.1894e-02,  1.1570e-02,\n",
      "        -6.5014e-03, -7.7481e-03,  2.5111e-03,  2.6754e-02, -3.5307e-02,\n",
      "        -4.8541e-03, -1.5102e-02,  2.7366e-02,  6.5582e-03, -2.2903e-02,\n",
      "        -1.3477e-03,  3.0479e-03, -1.5628e-03,  1.3759e-02, -1.7926e-02,\n",
      "        -1.1033e-02, -1.7137e-02,  4.1750e-03, -1.0361e-02,  2.6291e-02,\n",
      "         2.3286e-03,  1.2881e-02,  2.0004e-02,  1.1348e-02, -1.2897e-02,\n",
      "        -3.2694e-02,  2.8590e-02,  2.1377e-02, -3.4391e-02,  3.0823e-03,\n",
      "        -1.6637e-02,  3.0107e-02,  2.3640e-02,  2.1867e-02,  2.8707e-02,\n",
      "         3.4122e-02, -2.5247e-02, -1.9610e-02, -3.3860e-02,  3.2194e-02,\n",
      "        -2.6046e-03, -9.1005e-03, -2.1057e-02,  1.5792e-02,  3.1843e-02,\n",
      "        -3.3251e-02,  2.5247e-02,  2.4368e-02,  8.5513e-03,  2.7344e-02,\n",
      "        -3.3805e-02,  1.4193e-02,  2.3989e-02, -2.0907e-02,  1.0227e-02,\n",
      "        -4.8830e-03, -1.7780e-02, -1.5964e-02,  3.1371e-02, -1.1736e-02,\n",
      "         7.2089e-03,  1.4480e-02,  3.0707e-02,  1.7888e-02, -9.5322e-03,\n",
      "         1.3939e-03, -3.5344e-02, -1.4446e-02,  4.0338e-04,  2.5299e-02,\n",
      "        -2.9060e-02,  3.4618e-02,  3.0409e-02,  2.9207e-02, -4.2344e-03,\n",
      "         3.2538e-02, -8.9741e-03,  1.8168e-03,  1.8883e-02, -3.4104e-02,\n",
      "        -2.2782e-02,  2.5454e-02,  5.0598e-03, -1.8627e-02, -1.4743e-02,\n",
      "         2.1146e-03, -3.3488e-02,  2.2595e-02,  1.5476e-02,  1.6343e-02,\n",
      "        -1.8608e-02, -1.5343e-02,  4.4170e-03,  3.2968e-02,  2.6822e-02,\n",
      "         2.7876e-02,  1.9872e-02,  2.1525e-02, -1.8484e-02, -3.1182e-02,\n",
      "        -1.2004e-02,  1.6858e-03,  1.3175e-02,  2.1441e-02,  4.9802e-03,\n",
      "        -3.1303e-02, -1.4386e-02, -5.9870e-03, -1.3817e-02, -2.0525e-02,\n",
      "        -1.3443e-02,  2.6892e-02, -1.7206e-02,  3.4196e-02, -1.9149e-02,\n",
      "        -1.3897e-02,  6.4584e-03, -1.5424e-02,  2.3148e-03,  9.9244e-03,\n",
      "         2.8673e-02, -9.4887e-03, -1.6249e-02,  9.7089e-03, -2.7740e-02,\n",
      "         3.1033e-02, -1.0034e-02, -1.6177e-02, -7.8324e-03,  1.5194e-03],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-3.1971e-02,  2.7748e-02, -3.3335e-03,  ...,  5.3318e-03,\n",
      "          1.9834e-02, -1.8284e-02],\n",
      "        [ 3.2048e-02, -3.2285e-02,  4.0804e-03,  ..., -1.3745e-02,\n",
      "         -3.0004e-02, -3.1176e-02],\n",
      "        [ 4.2941e-02, -2.5332e-06,  2.4764e-02,  ..., -4.0345e-02,\n",
      "          8.7174e-03, -1.3680e-02],\n",
      "        ...,\n",
      "        [-2.4685e-02, -3.7837e-02,  5.1737e-03,  ..., -4.3555e-03,\n",
      "          4.4222e-02,  4.1758e-02],\n",
      "        [-1.0546e-02, -4.2310e-03,  2.6590e-02,  ...,  3.5582e-02,\n",
      "         -4.0400e-02,  6.6998e-03],\n",
      "        [-2.5550e-02, -3.7575e-02, -3.1275e-02,  ..., -4.8284e-03,\n",
      "          4.3188e-02, -6.0853e-03]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0280,  0.0149, -0.0265, -0.0311,  0.0125, -0.0205,  0.0232,  0.0420,\n",
      "         0.0246,  0.0249, -0.0289, -0.0167,  0.0152, -0.0120,  0.0390,  0.0028,\n",
      "         0.0024, -0.0182, -0.0386, -0.0037, -0.0042,  0.0155,  0.0409,  0.0328,\n",
      "        -0.0084, -0.0338,  0.0116,  0.0208,  0.0425,  0.0010,  0.0079, -0.0439,\n",
      "         0.0197, -0.0148,  0.0024,  0.0130, -0.0348,  0.0094,  0.0374, -0.0113,\n",
      "         0.0004,  0.0378,  0.0207, -0.0348, -0.0025, -0.0294,  0.0332,  0.0036,\n",
      "        -0.0405,  0.0070,  0.0038,  0.0247, -0.0284, -0.0159,  0.0369, -0.0108,\n",
      "         0.0034,  0.0305,  0.0288, -0.0240,  0.0293,  0.0040, -0.0421, -0.0128,\n",
      "        -0.0149,  0.0308,  0.0146, -0.0012,  0.0333,  0.0385, -0.0004,  0.0324,\n",
      "         0.0296, -0.0434, -0.0040, -0.0397,  0.0075, -0.0155, -0.0432,  0.0118,\n",
      "         0.0175, -0.0240, -0.0189,  0.0057, -0.0221,  0.0269,  0.0060, -0.0446,\n",
      "         0.0136, -0.0070,  0.0359,  0.0197,  0.0294, -0.0263, -0.0408,  0.0393,\n",
      "        -0.0119, -0.0046, -0.0318, -0.0014, -0.0393, -0.0181, -0.0186,  0.0196,\n",
      "         0.0305,  0.0285,  0.0301, -0.0119, -0.0147,  0.0192,  0.0225,  0.0025,\n",
      "         0.0212,  0.0370,  0.0248, -0.0073,  0.0184,  0.0293, -0.0270, -0.0232,\n",
      "         0.0293,  0.0087,  0.0209,  0.0127, -0.0372,  0.0147, -0.0415,  0.0092,\n",
      "        -0.0244, -0.0161, -0.0044, -0.0312, -0.0066, -0.0218,  0.0393,  0.0400,\n",
      "        -0.0054,  0.0246, -0.0399,  0.0426, -0.0050, -0.0323, -0.0287,  0.0349,\n",
      "        -0.0095,  0.0314, -0.0264, -0.0435,  0.0365, -0.0296,  0.0248, -0.0306,\n",
      "        -0.0306, -0.0102, -0.0326, -0.0173,  0.0361,  0.0406,  0.0171,  0.0426,\n",
      "        -0.0041,  0.0400, -0.0437, -0.0026,  0.0052, -0.0078, -0.0304,  0.0056,\n",
      "        -0.0247,  0.0380, -0.0262,  0.0248,  0.0035,  0.0296,  0.0060, -0.0285,\n",
      "         0.0119,  0.0355,  0.0072, -0.0142,  0.0335, -0.0116,  0.0247, -0.0039,\n",
      "        -0.0308, -0.0412,  0.0039,  0.0346,  0.0178,  0.0174,  0.0360,  0.0146,\n",
      "         0.0229,  0.0231,  0.0439, -0.0140,  0.0125, -0.0062, -0.0309,  0.0121,\n",
      "         0.0125, -0.0179,  0.0199,  0.0120, -0.0025,  0.0426, -0.0064,  0.0043,\n",
      "        -0.0385,  0.0062, -0.0432,  0.0302,  0.0136,  0.0123,  0.0204,  0.0170,\n",
      "         0.0111, -0.0059,  0.0067, -0.0018, -0.0284,  0.0072, -0.0183,  0.0167,\n",
      "         0.0058, -0.0261,  0.0097, -0.0241,  0.0208, -0.0286, -0.0236,  0.0416,\n",
      "         0.0312,  0.0285,  0.0414,  0.0168, -0.0331, -0.0383, -0.0329,  0.0077,\n",
      "         0.0039, -0.0118, -0.0039,  0.0420,  0.0243,  0.0231,  0.0236,  0.0288,\n",
      "        -0.0187,  0.0167, -0.0387,  0.0055, -0.0396,  0.0369, -0.0107, -0.0389,\n",
      "         0.0088, -0.0369,  0.0328,  0.0327, -0.0066,  0.0443,  0.0034,  0.0287,\n",
      "        -0.0158,  0.0065, -0.0377,  0.0117, -0.0407, -0.0067,  0.0122, -0.0170,\n",
      "         0.0424,  0.0246,  0.0100, -0.0159, -0.0376, -0.0170, -0.0118, -0.0261,\n",
      "         0.0300, -0.0124,  0.0352, -0.0145,  0.0083, -0.0127, -0.0282,  0.0192,\n",
      "        -0.0094, -0.0064, -0.0037,  0.0026, -0.0010,  0.0370, -0.0080,  0.0402,\n",
      "         0.0300,  0.0140,  0.0397, -0.0281], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0204, -0.0355, -0.0414,  ..., -0.0244, -0.0551, -0.0562],\n",
      "        [-0.0568, -0.0552, -0.0327,  ...,  0.0030, -0.0546,  0.0366],\n",
      "        [ 0.0338, -0.0326,  0.0357,  ...,  0.0113,  0.0113,  0.0005],\n",
      "        ...,\n",
      "        [-0.0300,  0.0088,  0.0521,  ..., -0.0368, -0.0490,  0.0091],\n",
      "        [-0.0014,  0.0530, -0.0387,  ...,  0.0022,  0.0142,  0.0458],\n",
      "        [ 0.0495, -0.0393, -0.0112,  ..., -0.0466, -0.0091, -0.0150]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0356,  0.0566, -0.0361,  0.0154,  0.0334, -0.0241,  0.0478, -0.0529,\n",
      "         0.0168, -0.0491], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.named_parameters():\n",
    "    print(p[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Eval Loop\n",
    "Train and Test functions are defined in the train_utils in the application folder. Contains functions that perform training and optimizer updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.233152\n",
      "Train Epoch: 1 [320/60000 (1%)]\tLoss: 2.231630\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.250200\n",
      "Train Epoch: 1 [960/60000 (2%)]\tLoss: 2.224289\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.248639\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 2.236761\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.203970\n",
      "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 2.194450\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.223135\n",
      "Train Epoch: 1 [2880/60000 (5%)]\tLoss: 2.256704\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.237671\n",
      "Train Epoch: 1 [3520/60000 (6%)]\tLoss: 2.215025\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.229399\n",
      "Train Epoch: 1 [4160/60000 (7%)]\tLoss: 2.176420\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.238171\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 2.175668\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.223419\n",
      "Train Epoch: 1 [5440/60000 (9%)]\tLoss: 2.225205\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.252855\n",
      "Train Epoch: 1 [6080/60000 (10%)]\tLoss: 2.203196\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.209046\n",
      "Train Epoch: 1 [6720/60000 (11%)]\tLoss: 2.209486\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.218561\n",
      "Train Epoch: 1 [7360/60000 (12%)]\tLoss: 2.238401\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.216637\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.196712\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.203145\n",
      "Train Epoch: 1 [8640/60000 (14%)]\tLoss: 2.193231\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.214417\n",
      "Train Epoch: 1 [9280/60000 (15%)]\tLoss: 2.237578\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.245378\n",
      "Train Epoch: 1 [9920/60000 (17%)]\tLoss: 2.206134\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.207602\n",
      "Train Epoch: 1 [10560/60000 (18%)]\tLoss: 2.163178\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.191159\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 2.208224\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.196406\n",
      "Train Epoch: 1 [11840/60000 (20%)]\tLoss: 2.211004\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.223031\n",
      "Train Epoch: 1 [12480/60000 (21%)]\tLoss: 2.156553\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.215683\n",
      "Train Epoch: 1 [13120/60000 (22%)]\tLoss: 2.180542\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.188625\n",
      "Train Epoch: 1 [13760/60000 (23%)]\tLoss: 2.165015\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.214000\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 2.136153\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.213192\n",
      "Train Epoch: 1 [15040/60000 (25%)]\tLoss: 2.213996\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.170100\n",
      "Train Epoch: 1 [15680/60000 (26%)]\tLoss: 2.152725\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.178077\n",
      "Train Epoch: 1 [16320/60000 (27%)]\tLoss: 2.151555\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.205211\n",
      "Train Epoch: 1 [16960/60000 (28%)]\tLoss: 2.139616\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.174107\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 2.156445\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.147105\n",
      "Train Epoch: 1 [18240/60000 (30%)]\tLoss: 2.168307\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.152020\n",
      "Train Epoch: 1 [18880/60000 (31%)]\tLoss: 2.133894\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.123554\n",
      "Train Epoch: 1 [19520/60000 (33%)]\tLoss: 2.188247\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.121424\n",
      "Train Epoch: 1 [20160/60000 (34%)]\tLoss: 2.127476\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.125235\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 2.157653\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.140362\n",
      "Train Epoch: 1 [21440/60000 (36%)]\tLoss: 2.091859\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.092587\n",
      "Train Epoch: 1 [22080/60000 (37%)]\tLoss: 2.102875\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.146591\n",
      "Train Epoch: 1 [22720/60000 (38%)]\tLoss: 2.161348\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.048733\n",
      "Train Epoch: 1 [23360/60000 (39%)]\tLoss: 2.176792\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.161176\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 2.111150\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.079311\n",
      "Train Epoch: 1 [24640/60000 (41%)]\tLoss: 2.095440\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.122910\n",
      "Train Epoch: 1 [25280/60000 (42%)]\tLoss: 2.133536\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.022743\n",
      "Train Epoch: 1 [25920/60000 (43%)]\tLoss: 2.139925\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.111719\n",
      "Train Epoch: 1 [26560/60000 (44%)]\tLoss: 2.093681\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.070288\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 2.109768\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.980008\n",
      "Train Epoch: 1 [27840/60000 (46%)]\tLoss: 2.013789\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.050698\n",
      "Train Epoch: 1 [28480/60000 (47%)]\tLoss: 1.995035\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.101099\n",
      "Train Epoch: 1 [29120/60000 (49%)]\tLoss: 1.978093\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.050321\n",
      "Train Epoch: 1 [29760/60000 (50%)]\tLoss: 2.006792\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.964960\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 2.024013\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.094997\n",
      "Train Epoch: 1 [31040/60000 (52%)]\tLoss: 2.005297\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.030056\n",
      "Train Epoch: 1 [31680/60000 (53%)]\tLoss: 2.065073\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.974103\n",
      "Train Epoch: 1 [32320/60000 (54%)]\tLoss: 2.008337\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.028283\n",
      "Train Epoch: 1 [32960/60000 (55%)]\tLoss: 2.014022\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.987640\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 1.958232\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.998724\n",
      "Train Epoch: 1 [34240/60000 (57%)]\tLoss: 2.069669\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.993323\n",
      "Train Epoch: 1 [34880/60000 (58%)]\tLoss: 2.011995\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.063564\n",
      "Train Epoch: 1 [35520/60000 (59%)]\tLoss: 1.944980\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.955970\n",
      "Train Epoch: 1 [36160/60000 (60%)]\tLoss: 1.932300\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 1.969523\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 1.955847\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.902199\n",
      "Train Epoch: 1 [37440/60000 (62%)]\tLoss: 1.951932\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.916879\n",
      "Train Epoch: 1 [38080/60000 (63%)]\tLoss: 1.933015\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.965392\n",
      "Train Epoch: 1 [38720/60000 (64%)]\tLoss: 1.929302\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 1.886247\n",
      "Train Epoch: 1 [39360/60000 (66%)]\tLoss: 1.960691\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.972568\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.854457\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.020576\n",
      "Train Epoch: 1 [40640/60000 (68%)]\tLoss: 1.953655\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.995084\n",
      "Train Epoch: 1 [41280/60000 (69%)]\tLoss: 1.871807\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.989638\n",
      "Train Epoch: 1 [41920/60000 (70%)]\tLoss: 1.886708\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.910138\n",
      "Train Epoch: 1 [42560/60000 (71%)]\tLoss: 1.901951\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.874540\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 1.810771\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.926224\n",
      "Train Epoch: 1 [43840/60000 (73%)]\tLoss: 1.912475\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.874799\n",
      "Train Epoch: 1 [44480/60000 (74%)]\tLoss: 1.888985\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.831474\n",
      "Train Epoch: 1 [45120/60000 (75%)]\tLoss: 1.934657\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.887746\n",
      "Train Epoch: 1 [45760/60000 (76%)]\tLoss: 1.811357\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.964093\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 1.856023\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.889002\n",
      "Train Epoch: 1 [47040/60000 (78%)]\tLoss: 1.876771\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.872228\n",
      "Train Epoch: 1 [47680/60000 (79%)]\tLoss: 1.924122\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.858149\n",
      "Train Epoch: 1 [48320/60000 (80%)]\tLoss: 1.885442\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.807307\n",
      "Train Epoch: 1 [48960/60000 (82%)]\tLoss: 1.851874\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.873953\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 1.860909\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.815125\n",
      "Train Epoch: 1 [50240/60000 (84%)]\tLoss: 1.935209\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.842846\n",
      "Train Epoch: 1 [50880/60000 (85%)]\tLoss: 1.919911\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.878832\n",
      "Train Epoch: 1 [51520/60000 (86%)]\tLoss: 1.906602\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.823910\n",
      "Train Epoch: 1 [52160/60000 (87%)]\tLoss: 1.954982\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.894743\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 1.800957\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.842905\n",
      "Train Epoch: 1 [53440/60000 (89%)]\tLoss: 1.846296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.883418\n",
      "Train Epoch: 1 [54080/60000 (90%)]\tLoss: 1.807143\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.827856\n",
      "Train Epoch: 1 [54720/60000 (91%)]\tLoss: 1.843085\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.850072\n",
      "Train Epoch: 1 [55360/60000 (92%)]\tLoss: 1.793453\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.850483\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.795912\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.862932\n",
      "Train Epoch: 1 [56640/60000 (94%)]\tLoss: 1.918845\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.894166\n",
      "Train Epoch: 1 [57280/60000 (95%)]\tLoss: 1.760769\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.867373\n",
      "Train Epoch: 1 [57920/60000 (96%)]\tLoss: 1.854577\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.824781\n",
      "Train Epoch: 1 [58560/60000 (98%)]\tLoss: 1.787142\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.795445\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 1.871804\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.880174\n",
      "Train Epoch: 1 [59840/60000 (100%)]\tLoss: 1.780052\n",
      "\n",
      "Test set: Avg. loss: 0.0018, Accuracy: 7251/10000 (73%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.806002\n",
      "Train Epoch: 2 [320/60000 (1%)]\tLoss: 1.780065\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.812744\n",
      "Train Epoch: 2 [960/60000 (2%)]\tLoss: 1.770494\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.795785\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 1.798163\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.868320\n",
      "Train Epoch: 2 [2240/60000 (4%)]\tLoss: 1.847607\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.840808\n",
      "Train Epoch: 2 [2880/60000 (5%)]\tLoss: 1.770920\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.770763\n",
      "Train Epoch: 2 [3520/60000 (6%)]\tLoss: 1.792128\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.867485\n",
      "Train Epoch: 2 [4160/60000 (7%)]\tLoss: 1.803382\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.820243\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 1.793738\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.820284\n",
      "Train Epoch: 2 [5440/60000 (9%)]\tLoss: 1.816696\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.851033\n",
      "Train Epoch: 2 [6080/60000 (10%)]\tLoss: 1.774532\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.714275\n",
      "Train Epoch: 2 [6720/60000 (11%)]\tLoss: 1.772051\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.738448\n",
      "Train Epoch: 2 [7360/60000 (12%)]\tLoss: 1.867323\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.802829\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 1.795768\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 1.869469\n",
      "Train Epoch: 2 [8640/60000 (14%)]\tLoss: 1.848730\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 1.733424\n",
      "Train Epoch: 2 [9280/60000 (15%)]\tLoss: 1.835946\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 1.752239\n",
      "Train Epoch: 2 [9920/60000 (17%)]\tLoss: 1.862278\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.820212\n",
      "Train Epoch: 2 [10560/60000 (18%)]\tLoss: 1.791648\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.730292\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 1.766772\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 1.787809\n",
      "Train Epoch: 2 [11840/60000 (20%)]\tLoss: 1.864963\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.711253\n",
      "Train Epoch: 2 [12480/60000 (21%)]\tLoss: 1.809555\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.779493\n",
      "Train Epoch: 2 [13120/60000 (22%)]\tLoss: 1.811610\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 1.795644\n",
      "Train Epoch: 2 [13760/60000 (23%)]\tLoss: 1.773669\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 1.797421\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 1.830194\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 1.752230\n",
      "Train Epoch: 2 [15040/60000 (25%)]\tLoss: 1.816508\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.785229\n",
      "Train Epoch: 2 [15680/60000 (26%)]\tLoss: 1.751821\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.757073\n",
      "Train Epoch: 2 [16320/60000 (27%)]\tLoss: 1.723522\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 1.758875\n",
      "Train Epoch: 2 [16960/60000 (28%)]\tLoss: 1.844261\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 1.873159\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 1.797125\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 1.785808\n",
      "Train Epoch: 2 [18240/60000 (30%)]\tLoss: 1.792172\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 1.801335\n",
      "Train Epoch: 2 [18880/60000 (31%)]\tLoss: 1.931724\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.783553\n",
      "Train Epoch: 2 [19520/60000 (33%)]\tLoss: 1.744604\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 1.780719\n",
      "Train Epoch: 2 [20160/60000 (34%)]\tLoss: 1.756658\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 1.767939\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 1.699595\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 1.756220\n",
      "Train Epoch: 2 [21440/60000 (36%)]\tLoss: 1.760983\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 1.735778\n",
      "Train Epoch: 2 [22080/60000 (37%)]\tLoss: 1.722955\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 1.728812\n",
      "Train Epoch: 2 [22720/60000 (38%)]\tLoss: 1.787476\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 1.820068\n",
      "Train Epoch: 2 [23360/60000 (39%)]\tLoss: 1.818158\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 1.766332\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 1.763849\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 1.701003\n",
      "Train Epoch: 2 [24640/60000 (41%)]\tLoss: 1.783924\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 1.714859\n",
      "Train Epoch: 2 [25280/60000 (42%)]\tLoss: 1.775130\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.848764\n",
      "Train Epoch: 2 [25920/60000 (43%)]\tLoss: 1.736794\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.818036\n",
      "Train Epoch: 2 [26560/60000 (44%)]\tLoss: 1.771399\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 1.858114\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 1.752746\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 1.812397\n",
      "Train Epoch: 2 [27840/60000 (46%)]\tLoss: 1.733485\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 1.798433\n",
      "Train Epoch: 2 [28480/60000 (47%)]\tLoss: 1.687499\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 1.755063\n",
      "Train Epoch: 2 [29120/60000 (49%)]\tLoss: 1.746590\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 1.717081\n",
      "Train Epoch: 2 [29760/60000 (50%)]\tLoss: 1.732237\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 1.801262\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 1.740884\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 1.822533\n",
      "Train Epoch: 2 [31040/60000 (52%)]\tLoss: 1.823126\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 1.721585\n",
      "Train Epoch: 2 [31680/60000 (53%)]\tLoss: 1.717113\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.760115\n",
      "Train Epoch: 2 [32320/60000 (54%)]\tLoss: 1.823788\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 1.680722\n",
      "Train Epoch: 2 [32960/60000 (55%)]\tLoss: 1.868609\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 1.694414\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 1.785062\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 1.845990\n",
      "Train Epoch: 2 [34240/60000 (57%)]\tLoss: 1.706707\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 1.693104\n",
      "Train Epoch: 2 [34880/60000 (58%)]\tLoss: 1.754444\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 1.746472\n",
      "Train Epoch: 2 [35520/60000 (59%)]\tLoss: 1.644164\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 1.794207\n",
      "Train Epoch: 2 [36160/60000 (60%)]\tLoss: 1.764810\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 1.732509\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 1.738523\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 1.780032\n",
      "Train Epoch: 2 [37440/60000 (62%)]\tLoss: 1.846884\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 1.763797\n",
      "Train Epoch: 2 [38080/60000 (63%)]\tLoss: 1.721439\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.833156\n",
      "Train Epoch: 2 [38720/60000 (64%)]\tLoss: 1.719616\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 1.742396\n",
      "Train Epoch: 2 [39360/60000 (66%)]\tLoss: 1.673518\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 1.726720\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.690346\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 1.704093\n",
      "Train Epoch: 2 [40640/60000 (68%)]\tLoss: 1.815566\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 1.716708\n",
      "Train Epoch: 2 [41280/60000 (69%)]\tLoss: 1.752232\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 1.676433\n",
      "Train Epoch: 2 [41920/60000 (70%)]\tLoss: 1.706100\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 1.731364\n",
      "Train Epoch: 2 [42560/60000 (71%)]\tLoss: 1.796357\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 1.765380\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 1.735350\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 1.781787\n",
      "Train Epoch: 2 [43840/60000 (73%)]\tLoss: 1.730106\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 1.771182\n",
      "Train Epoch: 2 [44480/60000 (74%)]\tLoss: 1.737244\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.715563\n",
      "Train Epoch: 2 [45120/60000 (75%)]\tLoss: 1.823338\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 1.791019\n",
      "Train Epoch: 2 [45760/60000 (76%)]\tLoss: 1.775506\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 1.851408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 1.710801\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 1.720500\n",
      "Train Epoch: 2 [47040/60000 (78%)]\tLoss: 1.708700\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 1.780888\n",
      "Train Epoch: 2 [47680/60000 (79%)]\tLoss: 1.839134\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.777441\n",
      "Train Epoch: 2 [48320/60000 (80%)]\tLoss: 1.718409\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 1.652351\n",
      "Train Epoch: 2 [48960/60000 (82%)]\tLoss: 1.680904\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 1.706476\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 1.759044\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 1.725934\n",
      "Train Epoch: 2 [50240/60000 (84%)]\tLoss: 1.751448\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 1.737457\n",
      "Train Epoch: 2 [50880/60000 (85%)]\tLoss: 1.679812\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.762359\n",
      "Train Epoch: 2 [51520/60000 (86%)]\tLoss: 1.777373\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 1.704179\n",
      "Train Epoch: 2 [52160/60000 (87%)]\tLoss: 1.738679\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 1.663093\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 1.736264\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 1.740309\n",
      "Train Epoch: 2 [53440/60000 (89%)]\tLoss: 1.797709\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 1.767066\n",
      "Train Epoch: 2 [54080/60000 (90%)]\tLoss: 1.795460\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 1.791271\n",
      "Train Epoch: 2 [54720/60000 (91%)]\tLoss: 1.649734\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 1.683632\n",
      "Train Epoch: 2 [55360/60000 (92%)]\tLoss: 1.715769\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 1.809985\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 1.659391\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 1.724526\n",
      "Train Epoch: 2 [56640/60000 (94%)]\tLoss: 1.731606\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 1.675191\n",
      "Train Epoch: 2 [57280/60000 (95%)]\tLoss: 1.651660\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.693316\n",
      "Train Epoch: 2 [57920/60000 (96%)]\tLoss: 1.700972\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 1.733094\n",
      "Train Epoch: 2 [58560/60000 (98%)]\tLoss: 1.664286\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 1.763012\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 1.674680\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 1.725188\n",
      "Train Epoch: 2 [59840/60000 (100%)]\tLoss: 1.712766\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 8143/10000 (81%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "  train_utils.train(model, optimizer, criterion, epoch, train_loader)\n",
    "  train_utils.test(model, criterion, test_loader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Conductance is used to find the Importance of Neurons within that layer.\n",
    "From Captum docs: Layer attributions allow us to understand the importance of all the neurons in the output of a particular layer. For this example, we will be using Layer Conductance, one of the Layer Attribution methods in Captum, which is an extension of Integrated Gradients applied to hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0346,  0.0045, -0.0198,  ...,  0.0288, -0.0244,  0.0161],\n",
      "        [-0.0361,  0.0287, -0.0068,  ..., -0.0225,  0.0216,  0.0335],\n",
      "        [ 0.0054,  0.0139,  0.0004,  ...,  0.0099,  0.0051,  0.0182],\n",
      "        ...,\n",
      "        [-0.0002, -0.0184, -0.0152,  ...,  0.0221,  0.0144, -0.0189],\n",
      "        [ 0.0105,  0.0276,  0.0127,  ...,  0.0112, -0.0155,  0.0221],\n",
      "        [-0.0243,  0.0103, -0.0174,  ...,  0.0062,  0.0105, -0.0133]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0219,  0.0104, -0.0306,  0.0311, -0.0335,  0.0216, -0.0069, -0.0157,\n",
      "        -0.0028,  0.0310,  0.0096, -0.0180, -0.0333,  0.0242,  0.0241,  0.0406,\n",
      "         0.0006,  0.0067, -0.0088,  0.0318, -0.0010,  0.0146,  0.0223, -0.0104,\n",
      "        -0.0318, -0.0173,  0.0095, -0.0136,  0.0111,  0.0004, -0.0100,  0.0301,\n",
      "        -0.0261,  0.0077, -0.0319,  0.0165,  0.0005, -0.0290,  0.0377,  0.0176,\n",
      "         0.0206, -0.0130, -0.0128, -0.0097,  0.0270,  0.0087, -0.0044,  0.0343,\n",
      "        -0.0223,  0.0164,  0.0248,  0.0281,  0.0300, -0.0047,  0.0293, -0.0265,\n",
      "        -0.0341, -0.0321,  0.0160,  0.0227,  0.0039, -0.0266, -0.0315, -0.0244,\n",
      "        -0.0061, -0.0091,  0.0003,  0.0293, -0.0295, -0.0216, -0.0209, -0.0214,\n",
      "         0.0184,  0.0305,  0.0373,  0.0202, -0.0186, -0.0310, -0.0058,  0.0145,\n",
      "        -0.0020,  0.0166, -0.0069,  0.0140, -0.0152,  0.0315, -0.0053,  0.0274,\n",
      "         0.0170, -0.0136, -0.0031, -0.0084, -0.0185,  0.0281, -0.0087, -0.0307,\n",
      "         0.0053, -0.0345,  0.0015,  0.0037,  0.0192,  0.0292,  0.0359, -0.0261,\n",
      "        -0.0066,  0.0279,  0.0081,  0.0142,  0.0295, -0.0159,  0.0058, -0.0123,\n",
      "        -0.0050,  0.0215, -0.0227,  0.0281,  0.0322,  0.0268,  0.0191, -0.0075,\n",
      "        -0.0163, -0.0157, -0.0028,  0.0286,  0.0014, -0.0273, -0.0028,  0.0119,\n",
      "         0.0304,  0.0184,  0.0079,  0.0179, -0.0317,  0.0343,  0.0102,  0.0323,\n",
      "        -0.0273,  0.0328,  0.0092, -0.0231, -0.0280, -0.0061,  0.0317, -0.0280,\n",
      "         0.0156,  0.0390, -0.0081,  0.0125, -0.0147,  0.0143, -0.0269,  0.0076,\n",
      "        -0.0141,  0.0032,  0.0307,  0.0253, -0.0160,  0.0209, -0.0205, -0.0118,\n",
      "         0.0237,  0.0076, -0.0264, -0.0054, -0.0267,  0.0053,  0.0369,  0.0237,\n",
      "         0.0258,  0.0117,  0.0166, -0.0144, -0.0277,  0.0007, -0.0284,  0.0143,\n",
      "        -0.0232,  0.0088,  0.0113,  0.0118, -0.0145,  0.0071, -0.0202,  0.0177,\n",
      "        -0.0283,  0.0283,  0.0242, -0.0122,  0.0324,  0.0027, -0.0352, -0.0132,\n",
      "         0.0365, -0.0110, -0.0062, -0.0052,  0.0321, -0.0130, -0.0008,  0.0238,\n",
      "         0.0070, -0.0046, -0.0229,  0.0126,  0.0187,  0.0111, -0.0065, -0.0208,\n",
      "        -0.0074,  0.0306,  0.0152,  0.0248, -0.0202, -0.0016, -0.0051,  0.0308,\n",
      "        -0.0137,  0.0306,  0.0222,  0.0346, -0.0322,  0.0163, -0.0161,  0.0327,\n",
      "         0.0248, -0.0040,  0.0144,  0.0321, -0.0060,  0.0269,  0.0035, -0.0016,\n",
      "        -0.0248,  0.0346,  0.0404, -0.0233,  0.0253, -0.0081, -0.0198, -0.0123,\n",
      "         0.0006,  0.0189,  0.0239,  0.0012,  0.0301, -0.0339,  0.0102, -0.0229,\n",
      "         0.0326,  0.0319, -0.0279,  0.0355,  0.0318, -0.0074, -0.0192, -0.0103,\n",
      "        -0.0112,  0.0337, -0.0153, -0.0097, -0.0095,  0.0221, -0.0115,  0.0118,\n",
      "        -0.0144, -0.0297,  0.0324,  0.0252,  0.0157,  0.0188,  0.0093,  0.0113,\n",
      "        -0.0106,  0.0001, -0.0255, -0.0324, -0.0216,  0.0096, -0.0112, -0.0296,\n",
      "         0.0170,  0.0278,  0.0352,  0.0274, -0.0153, -0.0162, -0.0312,  0.0318,\n",
      "         0.0291,  0.0048,  0.0341, -0.0323, -0.0314, -0.0161, -0.0054, -0.0154,\n",
      "        -0.0326, -0.0219,  0.0328,  0.0328, -0.0261, -0.0246,  0.0025, -0.0024,\n",
      "         0.0231, -0.0235,  0.0026, -0.0231, -0.0027,  0.0180,  0.0100,  0.0295,\n",
      "        -0.0225, -0.0180, -0.0207,  0.0156,  0.0278, -0.0164,  0.0115,  0.0185,\n",
      "         0.0091,  0.0147,  0.0003,  0.0246,  0.0092, -0.0052, -0.0188, -0.0025,\n",
      "         0.0264, -0.0078, -0.0149, -0.0154, -0.0086, -0.0286,  0.0172,  0.0017,\n",
      "         0.0185, -0.0005, -0.0219,  0.0342,  0.0115,  0.0283, -0.0161, -0.0211,\n",
      "         0.0221, -0.0340,  0.0314, -0.0042, -0.0145,  0.0011, -0.0003,  0.0334,\n",
      "        -0.0144,  0.0065, -0.0278, -0.0331, -0.0293,  0.0119,  0.0009, -0.0336,\n",
      "         0.0167,  0.0132, -0.0271,  0.0309, -0.0307, -0.0217,  0.0274,  0.0088,\n",
      "         0.0061,  0.0365, -0.0089, -0.0276,  0.0298,  0.0373,  0.0071, -0.0324,\n",
      "        -0.0027, -0.0273,  0.0033,  0.0065,  0.0090,  0.0306,  0.0176, -0.0338,\n",
      "        -0.0090,  0.0067, -0.0255,  0.0094,  0.0300,  0.0023,  0.0118, -0.0315,\n",
      "        -0.0071, -0.0147, -0.0303, -0.0223,  0.0324, -0.0282,  0.0068,  0.0196,\n",
      "         0.0061,  0.0104, -0.0282,  0.0078,  0.0145,  0.0353, -0.0116, -0.0079,\n",
      "         0.0041, -0.0144, -0.0068, -0.0250,  0.0114, -0.0109, -0.0362, -0.0077,\n",
      "        -0.0195, -0.0053, -0.0158,  0.0167,  0.0084, -0.0093,  0.0258, -0.0044,\n",
      "        -0.0219,  0.0036, -0.0082,  0.0303,  0.0083, -0.0179,  0.0205,  0.0243,\n",
      "        -0.0228,  0.0168, -0.0092, -0.0173, -0.0187, -0.0296,  0.0244, -0.0249,\n",
      "         0.0297,  0.0260, -0.0004, -0.0236,  0.0322, -0.0300,  0.0264,  0.0183,\n",
      "        -0.0078,  0.0221, -0.0146,  0.0284,  0.0270,  0.0274,  0.0079,  0.0148,\n",
      "        -0.0072,  0.0199, -0.0089,  0.0182,  0.0305, -0.0040,  0.0278,  0.0144,\n",
      "         0.0062, -0.0194, -0.0063, -0.0078, -0.0338,  0.0112, -0.0047,  0.0184,\n",
      "        -0.0171,  0.0021, -0.0258, -0.0018,  0.0158,  0.0143, -0.0228,  0.0331,\n",
      "         0.0319,  0.0398,  0.0145, -0.0139,  0.0387,  0.0250,  0.0252,  0.0352,\n",
      "        -0.0302, -0.0030, -0.0043, -0.0049,  0.0025, -0.0004,  0.0098,  0.0175,\n",
      "         0.0054,  0.0067,  0.0217,  0.0227], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0451,  0.0054, -0.0354,  ..., -0.0207,  0.0126,  0.0447],\n",
      "        [-0.0182, -0.0130,  0.0087,  ...,  0.0213,  0.0382, -0.0245],\n",
      "        [ 0.0174, -0.0336,  0.0367,  ..., -0.0247, -0.0018,  0.0108],\n",
      "        ...,\n",
      "        [ 0.0352,  0.0260,  0.0013,  ...,  0.0115, -0.0147, -0.0110],\n",
      "        [ 0.0249,  0.0025,  0.0032,  ..., -0.0372,  0.0142, -0.0150],\n",
      "        [-0.0272, -0.0387, -0.0012,  ...,  0.0072, -0.0366, -0.0287]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0319,  0.0165,  0.0100,  0.0079,  0.0203, -0.0125, -0.0249,  0.0481,\n",
      "         0.0295, -0.0110,  0.0179, -0.0082,  0.0532,  0.0243, -0.0231,  0.0133,\n",
      "         0.0377,  0.0473,  0.0431,  0.0471,  0.0153,  0.0297, -0.0395, -0.0395,\n",
      "        -0.0320,  0.0009,  0.0474,  0.0094, -0.0111, -0.0414, -0.0034, -0.0203,\n",
      "        -0.0370, -0.0413, -0.0302, -0.0118, -0.0399,  0.0351, -0.0356, -0.0098,\n",
      "        -0.0098,  0.0199,  0.0367,  0.0197,  0.0133,  0.0305,  0.0269, -0.0231,\n",
      "        -0.0062, -0.0169, -0.0389,  0.0060,  0.0366, -0.0134,  0.0325,  0.0412,\n",
      "         0.0125, -0.0172,  0.0360, -0.0245,  0.0299, -0.0422,  0.0361,  0.0021,\n",
      "        -0.0335,  0.0022,  0.0138,  0.0156,  0.0332, -0.0391, -0.0377, -0.0296,\n",
      "        -0.0340, -0.0085,  0.0122,  0.0470,  0.0360,  0.0111,  0.0314, -0.0388,\n",
      "         0.0494, -0.0070,  0.0397, -0.0112,  0.0252,  0.0241,  0.0100, -0.0273,\n",
      "        -0.0089, -0.0160, -0.0316,  0.0129,  0.0023, -0.0445, -0.0233,  0.0400,\n",
      "        -0.0246,  0.0129,  0.0375, -0.0256,  0.0308,  0.0117,  0.0013,  0.0060,\n",
      "         0.0018, -0.0219, -0.0242,  0.0242,  0.0242,  0.0304,  0.0006,  0.0065,\n",
      "        -0.0104,  0.0269,  0.0316, -0.0023,  0.0313,  0.0072, -0.0376,  0.0182,\n",
      "        -0.0150, -0.0067,  0.0226,  0.0352, -0.0016,  0.0338, -0.0301,  0.0391,\n",
      "         0.0273,  0.0356,  0.0225, -0.0084,  0.0498,  0.0267, -0.0210, -0.0245,\n",
      "         0.0176, -0.0056, -0.0177,  0.0455, -0.0282, -0.0206,  0.0219,  0.0144,\n",
      "         0.0418,  0.0380,  0.0224, -0.0218,  0.0235, -0.0190,  0.0186, -0.0225,\n",
      "         0.0495, -0.0229, -0.0269, -0.0180, -0.0248, -0.0130, -0.0245, -0.0055,\n",
      "         0.0264, -0.0420, -0.0012, -0.0398,  0.0002, -0.0305, -0.0171, -0.0318,\n",
      "         0.0309, -0.0062,  0.0435,  0.0267,  0.0466, -0.0358,  0.0048, -0.0142,\n",
      "        -0.0375, -0.0103, -0.0361, -0.0221, -0.0440,  0.0431,  0.0433,  0.0064,\n",
      "        -0.0372,  0.0361,  0.0088, -0.0175,  0.0128,  0.0049,  0.0104,  0.0439,\n",
      "        -0.0355, -0.0035,  0.0123, -0.0355, -0.0324,  0.0315,  0.0017, -0.0310,\n",
      "        -0.0010, -0.0078, -0.0117, -0.0150, -0.0054, -0.0248, -0.0285,  0.0207,\n",
      "        -0.0018,  0.0447,  0.0371, -0.0335,  0.0471,  0.0189, -0.0038, -0.0164,\n",
      "        -0.0247,  0.0416, -0.0246,  0.0301,  0.0117,  0.0262, -0.0160, -0.0052,\n",
      "         0.0005, -0.0180, -0.0026,  0.0055, -0.0397, -0.0202, -0.0169,  0.0443,\n",
      "        -0.0276,  0.0314, -0.0186, -0.0091,  0.0254, -0.0070,  0.0414,  0.0481,\n",
      "         0.0390,  0.0218,  0.0375, -0.0126, -0.0142,  0.0015, -0.0168,  0.0126,\n",
      "        -0.0069,  0.0444,  0.0148, -0.0047, -0.0036,  0.0255, -0.0097,  0.0089,\n",
      "        -0.0057,  0.0151, -0.0091,  0.0396, -0.0343, -0.0039, -0.0139,  0.0028,\n",
      "        -0.0297, -0.0289, -0.0047,  0.0380,  0.0281, -0.0072,  0.0359, -0.0218,\n",
      "        -0.0065, -0.0301,  0.0453,  0.0375, -0.0207, -0.0112, -0.0204, -0.0194,\n",
      "         0.0321, -0.0161, -0.0095,  0.0269,  0.0139, -0.0264, -0.0102,  0.0131,\n",
      "         0.0296, -0.0352,  0.0079,  0.0507, -0.0018, -0.0333,  0.0196,  0.0475,\n",
      "         0.0413, -0.0375,  0.0229, -0.0156], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0225,  0.0467,  0.0367,  ..., -0.0503, -0.0365, -0.0241],\n",
      "        [-0.0490, -0.0644, -0.0748,  ..., -0.0234,  0.0564, -0.0705],\n",
      "        [-0.0308,  0.0083, -0.1463,  ...,  0.0519, -0.0240,  0.0474],\n",
      "        ...,\n",
      "        [ 0.0272, -0.0155,  0.1202,  ...,  0.0219,  0.0093,  0.0981],\n",
      "        [ 0.0593,  0.0070, -0.0161,  ..., -0.0141,  0.0018,  0.0234],\n",
      "        [-0.0400, -0.0107,  0.0940,  ..., -0.0019,  0.0398,  0.0511]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0174,  0.0186, -0.0421,  0.0406,  0.0450, -0.0063,  0.0079,  0.0661,\n",
      "         0.0330,  0.0038], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.named_parameters():\n",
    "    print(p[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=500, out_features=300, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = LayerConductance(model, model.input_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch of test data - number of samples defined in batch-size\n",
    "test_data, test_target = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the test_data tensor and the corresponding targets. could also be done over all possible targets and then classwise averages can be taken.\n",
    "cond_vals = cond.attribute(test_data,target=test_target)\n",
    "cond_vals = cond_vals.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_vals.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cond_vals, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.bar(x_pos, importances, align='center')\n",
    "        plt.xticks(x_pos, feature_names, wrap=True)\n",
    "        plt.xlabel(axis_title)\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Feature Importances\n",
      "0 :  0.001\n",
      "1 :  0.003\n",
      "2 :  0.002\n",
      "3 :  0.001\n",
      "4 :  0.000\n",
      "5 :  0.002\n",
      "6 :  0.000\n",
      "7 :  0.000\n",
      "8 :  0.004\n",
      "9 :  0.001\n",
      "10 :  0.000\n",
      "11 :  0.000\n",
      "12 :  0.002\n",
      "13 :  0.002\n",
      "14 :  0.000\n",
      "15 :  0.003\n",
      "16 :  0.000\n",
      "17 :  0.000\n",
      "18 :  0.001\n",
      "19 :  0.000\n",
      "20 :  0.001\n",
      "21 :  0.003\n",
      "22 :  0.002\n",
      "23 :  0.004\n",
      "24 :  -0.000\n",
      "25 :  0.001\n",
      "26 :  0.000\n",
      "27 :  0.001\n",
      "28 :  0.000\n",
      "29 :  0.001\n",
      "30 :  0.002\n",
      "31 :  0.002\n",
      "32 :  0.000\n",
      "33 :  0.002\n",
      "34 :  -0.000\n",
      "35 :  0.003\n",
      "36 :  0.002\n",
      "37 :  0.001\n",
      "38 :  0.001\n",
      "39 :  0.002\n",
      "40 :  0.000\n",
      "41 :  0.000\n",
      "42 :  0.000\n",
      "43 :  0.001\n",
      "44 :  0.004\n",
      "45 :  0.001\n",
      "46 :  0.000\n",
      "47 :  0.002\n",
      "48 :  0.000\n",
      "49 :  0.001\n",
      "50 :  0.002\n",
      "51 :  0.001\n",
      "52 :  0.000\n",
      "53 :  0.003\n",
      "54 :  0.001\n",
      "55 :  0.001\n",
      "56 :  -0.000\n",
      "57 :  0.002\n",
      "58 :  0.002\n",
      "59 :  0.003\n",
      "60 :  0.001\n",
      "61 :  0.002\n",
      "62 :  0.002\n",
      "63 :  0.002\n",
      "64 :  0.002\n",
      "65 :  0.000\n",
      "66 :  0.001\n",
      "67 :  0.001\n",
      "68 :  0.000\n",
      "69 :  0.003\n",
      "70 :  0.002\n",
      "71 :  0.000\n",
      "72 :  0.001\n",
      "73 :  0.001\n",
      "74 :  0.001\n",
      "75 :  0.004\n",
      "76 :  0.001\n",
      "77 :  0.002\n",
      "78 :  0.001\n",
      "79 :  0.004\n",
      "80 :  0.000\n",
      "81 :  0.000\n",
      "82 :  0.000\n",
      "83 :  0.002\n",
      "84 :  0.001\n",
      "85 :  0.001\n",
      "86 :  0.000\n",
      "87 :  0.002\n",
      "88 :  0.002\n",
      "89 :  0.003\n",
      "90 :  0.000\n",
      "91 :  0.000\n",
      "92 :  0.001\n",
      "93 :  0.000\n",
      "94 :  0.003\n",
      "95 :  0.002\n",
      "96 :  0.002\n",
      "97 :  0.000\n",
      "98 :  0.000\n",
      "99 :  0.001\n",
      "100 :  0.001\n",
      "101 :  0.001\n",
      "102 :  0.000\n",
      "103 :  0.000\n",
      "104 :  0.001\n",
      "105 :  0.000\n",
      "106 :  0.000\n",
      "107 :  0.003\n",
      "108 :  0.001\n",
      "109 :  0.002\n",
      "110 :  0.000\n",
      "111 :  0.003\n",
      "112 :  0.000\n",
      "113 :  0.001\n",
      "114 :  0.002\n",
      "115 :  0.001\n",
      "116 :  0.003\n",
      "117 :  0.003\n",
      "118 :  0.001\n",
      "119 :  0.000\n",
      "120 :  0.000\n",
      "121 :  0.003\n",
      "122 :  0.001\n",
      "123 :  0.005\n",
      "124 :  0.001\n",
      "125 :  0.004\n",
      "126 :  0.001\n",
      "127 :  0.001\n",
      "128 :  0.003\n",
      "129 :  0.003\n",
      "130 :  0.006\n",
      "131 :  0.004\n",
      "132 :  0.003\n",
      "133 :  0.001\n",
      "134 :  0.000\n",
      "135 :  -0.000\n",
      "136 :  0.001\n",
      "137 :  0.002\n",
      "138 :  0.000\n",
      "139 :  0.000\n",
      "140 :  0.001\n",
      "141 :  0.002\n",
      "142 :  0.002\n",
      "143 :  0.005\n",
      "144 :  0.003\n",
      "145 :  0.002\n",
      "146 :  0.001\n",
      "147 :  0.004\n",
      "148 :  0.003\n",
      "149 :  0.003\n",
      "150 :  0.001\n",
      "151 :  0.001\n",
      "152 :  0.001\n",
      "153 :  0.002\n",
      "154 :  0.002\n",
      "155 :  0.003\n",
      "156 :  0.005\n",
      "157 :  0.002\n",
      "158 :  0.001\n",
      "159 :  0.000\n",
      "160 :  0.001\n",
      "161 :  0.000\n",
      "162 :  0.001\n",
      "163 :  0.003\n",
      "164 :  0.001\n",
      "165 :  0.000\n",
      "166 :  0.004\n",
      "167 :  0.002\n",
      "168 :  0.001\n",
      "169 :  0.001\n",
      "170 :  0.001\n",
      "171 :  0.002\n",
      "172 :  0.001\n",
      "173 :  -0.000\n",
      "174 :  0.000\n",
      "175 :  0.009\n",
      "176 :  0.003\n",
      "177 :  0.000\n",
      "178 :  0.002\n",
      "179 :  0.002\n",
      "180 :  0.003\n",
      "181 :  0.000\n",
      "182 :  0.000\n",
      "183 :  0.003\n",
      "184 :  0.000\n",
      "185 :  0.002\n",
      "186 :  0.001\n",
      "187 :  0.000\n",
      "188 :  0.003\n",
      "189 :  0.001\n",
      "190 :  0.000\n",
      "191 :  0.001\n",
      "192 :  0.005\n",
      "193 :  0.000\n",
      "194 :  0.003\n",
      "195 :  0.000\n",
      "196 :  0.000\n",
      "197 :  0.002\n",
      "198 :  0.002\n",
      "199 :  0.005\n",
      "200 :  0.000\n",
      "201 :  0.001\n",
      "202 :  0.002\n",
      "203 :  -0.000\n",
      "204 :  0.000\n",
      "205 :  0.000\n",
      "206 :  0.003\n",
      "207 :  0.001\n",
      "208 :  0.000\n",
      "209 :  0.002\n",
      "210 :  0.001\n",
      "211 :  0.000\n",
      "212 :  0.001\n",
      "213 :  0.000\n",
      "214 :  0.005\n",
      "215 :  0.000\n",
      "216 :  0.001\n",
      "217 :  0.003\n",
      "218 :  0.000\n",
      "219 :  0.003\n",
      "220 :  0.000\n",
      "221 :  -0.000\n",
      "222 :  0.000\n",
      "223 :  0.002\n",
      "224 :  0.002\n",
      "225 :  -0.000\n",
      "226 :  0.002\n",
      "227 :  0.001\n",
      "228 :  0.002\n",
      "229 :  0.000\n",
      "230 :  -0.000\n",
      "231 :  0.001\n",
      "232 :  0.004\n",
      "233 :  0.000\n",
      "234 :  0.003\n",
      "235 :  0.002\n",
      "236 :  -0.000\n",
      "237 :  0.000\n",
      "238 :  0.002\n",
      "239 :  0.000\n",
      "240 :  0.001\n",
      "241 :  0.001\n",
      "242 :  0.001\n",
      "243 :  0.003\n",
      "244 :  0.001\n",
      "245 :  0.000\n",
      "246 :  0.000\n",
      "247 :  0.003\n",
      "248 :  0.002\n",
      "249 :  0.001\n",
      "250 :  0.005\n",
      "251 :  0.002\n",
      "252 :  0.001\n",
      "253 :  0.001\n",
      "254 :  0.000\n",
      "255 :  0.003\n",
      "256 :  0.002\n",
      "257 :  0.005\n",
      "258 :  0.001\n",
      "259 :  -0.000\n",
      "260 :  0.002\n",
      "261 :  0.000\n",
      "262 :  0.002\n",
      "263 :  0.000\n",
      "264 :  0.001\n",
      "265 :  0.002\n",
      "266 :  0.002\n",
      "267 :  0.001\n",
      "268 :  0.000\n",
      "269 :  0.001\n",
      "270 :  0.000\n",
      "271 :  0.000\n",
      "272 :  0.001\n",
      "273 :  0.003\n",
      "274 :  0.001\n",
      "275 :  0.000\n",
      "276 :  0.003\n",
      "277 :  0.001\n",
      "278 :  0.000\n",
      "279 :  0.000\n",
      "280 :  0.000\n",
      "281 :  0.001\n",
      "282 :  0.000\n",
      "283 :  0.001\n",
      "284 :  0.000\n",
      "285 :  0.002\n",
      "286 :  0.002\n",
      "287 :  0.001\n",
      "288 :  0.001\n",
      "289 :  0.000\n",
      "290 :  0.003\n",
      "291 :  0.000\n",
      "292 :  0.001\n",
      "293 :  0.003\n",
      "294 :  0.001\n",
      "295 :  0.001\n",
      "296 :  0.000\n",
      "297 :  0.001\n",
      "298 :  0.001\n",
      "299 :  0.001\n",
      "300 :  0.002\n",
      "301 :  0.006\n",
      "302 :  0.000\n",
      "303 :  0.000\n",
      "304 :  0.003\n",
      "305 :  0.002\n",
      "306 :  0.000\n",
      "307 :  0.001\n",
      "308 :  0.000\n",
      "309 :  0.000\n",
      "310 :  0.000\n",
      "311 :  0.001\n",
      "312 :  0.000\n",
      "313 :  0.001\n",
      "314 :  0.004\n",
      "315 :  0.002\n",
      "316 :  0.001\n",
      "317 :  0.002\n",
      "318 :  0.001\n",
      "319 :  0.001\n",
      "320 :  0.008\n",
      "321 :  0.000\n",
      "322 :  0.000\n",
      "323 :  0.001\n",
      "324 :  0.001\n",
      "325 :  -0.000\n",
      "326 :  0.000\n",
      "327 :  0.000\n",
      "328 :  0.000\n",
      "329 :  0.001\n",
      "330 :  0.002\n",
      "331 :  0.000\n",
      "332 :  0.000\n",
      "333 :  0.001\n",
      "334 :  0.000\n",
      "335 :  0.001\n",
      "336 :  0.000\n",
      "337 :  0.002\n",
      "338 :  0.002\n",
      "339 :  0.000\n",
      "340 :  0.001\n",
      "341 :  0.000\n",
      "342 :  0.002\n",
      "343 :  0.003\n",
      "344 :  0.000\n",
      "345 :  -0.000\n",
      "346 :  -0.000\n",
      "347 :  0.000\n",
      "348 :  0.001\n",
      "349 :  0.001\n",
      "350 :  0.000\n",
      "351 :  0.003\n",
      "352 :  0.002\n",
      "353 :  0.001\n",
      "354 :  0.000\n",
      "355 :  0.001\n",
      "356 :  0.002\n",
      "357 :  0.001\n",
      "358 :  0.002\n",
      "359 :  0.002\n",
      "360 :  0.003\n",
      "361 :  0.004\n",
      "362 :  0.001\n",
      "363 :  0.000\n",
      "364 :  0.000\n",
      "365 :  0.000\n",
      "366 :  0.001\n",
      "367 :  0.000\n",
      "368 :  0.001\n",
      "369 :  0.001\n",
      "370 :  0.003\n",
      "371 :  0.000\n",
      "372 :  0.002\n",
      "373 :  0.001\n",
      "374 :  0.000\n",
      "375 :  0.000\n",
      "376 :  0.001\n",
      "377 :  0.000\n",
      "378 :  0.001\n",
      "379 :  0.003\n",
      "380 :  0.004\n",
      "381 :  0.002\n",
      "382 :  0.001\n",
      "383 :  0.000\n",
      "384 :  0.000\n",
      "385 :  0.002\n",
      "386 :  0.004\n",
      "387 :  0.001\n",
      "388 :  0.001\n",
      "389 :  0.000\n",
      "390 :  0.000\n",
      "391 :  0.000\n",
      "392 :  0.003\n",
      "393 :  0.002\n",
      "394 :  0.001\n",
      "395 :  0.000\n",
      "396 :  0.001\n",
      "397 :  0.000\n",
      "398 :  0.002\n",
      "399 :  0.001\n",
      "400 :  0.002\n",
      "401 :  0.000\n",
      "402 :  0.000\n",
      "403 :  0.003\n",
      "404 :  -0.000\n",
      "405 :  0.002\n",
      "406 :  0.001\n",
      "407 :  0.003\n",
      "408 :  0.002\n",
      "409 :  0.003\n",
      "410 :  -0.000\n",
      "411 :  0.002\n",
      "412 :  0.001\n",
      "413 :  0.007\n",
      "414 :  0.000\n",
      "415 :  -0.000\n",
      "416 :  0.000\n",
      "417 :  -0.000\n",
      "418 :  0.000\n",
      "419 :  0.001\n",
      "420 :  0.002\n",
      "421 :  0.001\n",
      "422 :  0.001\n",
      "423 :  0.001\n",
      "424 :  0.002\n",
      "425 :  0.001\n",
      "426 :  0.001\n",
      "427 :  0.003\n",
      "428 :  0.000\n",
      "429 :  0.001\n",
      "430 :  0.000\n",
      "431 :  0.000\n",
      "432 :  0.000\n",
      "433 :  0.000\n",
      "434 :  0.002\n",
      "435 :  0.001\n",
      "436 :  0.003\n",
      "437 :  0.000\n",
      "438 :  0.001\n",
      "439 :  0.000\n",
      "440 :  0.004\n",
      "441 :  0.000\n",
      "442 :  0.001\n",
      "443 :  0.002\n",
      "444 :  0.001\n",
      "445 :  -0.000\n",
      "446 :  0.001\n",
      "447 :  0.001\n",
      "448 :  0.002\n",
      "449 :  0.000\n",
      "450 :  0.001\n",
      "451 :  0.003\n",
      "452 :  0.000\n",
      "453 :  0.004\n",
      "454 :  0.000\n",
      "455 :  -0.000\n",
      "456 :  0.001\n",
      "457 :  0.001\n",
      "458 :  0.003\n",
      "459 :  0.000\n",
      "460 :  0.000\n",
      "461 :  0.002\n",
      "462 :  0.000\n",
      "463 :  0.001\n",
      "464 :  0.007\n",
      "465 :  0.001\n",
      "466 :  0.001\n",
      "467 :  0.000\n",
      "468 :  0.000\n",
      "469 :  0.001\n",
      "470 :  0.001\n",
      "471 :  0.001\n",
      "472 :  0.001\n",
      "473 :  0.003\n",
      "474 :  0.001\n",
      "475 :  0.003\n",
      "476 :  0.005\n",
      "477 :  0.001\n",
      "478 :  0.000\n",
      "479 :  0.000\n",
      "480 :  0.000\n",
      "481 :  0.001\n",
      "482 :  0.004\n",
      "483 :  0.000\n",
      "484 :  0.003\n",
      "485 :  0.002\n",
      "486 :  0.000\n",
      "487 :  0.000\n",
      "488 :  0.000\n",
      "489 :  0.000\n",
      "490 :  0.002\n",
      "491 :  0.001\n",
      "492 :  0.000\n",
      "493 :  0.000\n",
      "494 :  0.001\n",
      "495 :  0.001\n",
      "496 :  0.002\n",
      "497 :  0.000\n",
      "498 :  0.001\n",
      "499 :  0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Range for number of hidden units in the layer\n",
    "visualize_importances(range(model.input_layer.out_features), np.mean(cond_vals, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
